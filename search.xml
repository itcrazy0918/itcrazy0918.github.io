<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Kafka-源码解析之-Broker文件存储（三）]]></title>
    <url>%2F2019%2F02%2F21%2FKafka-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8B-Broker-%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[摘要：消息存储对于每一款消息队列都非常重要，那么Kafka在这方面是如何来设计做到高效的呢？ Kafka这款分布式消息队列使用文件系统和操作系统的页缓存（page cache）分别存储和缓存消息，摒弃了Java的堆缓存机制，同时将随机写操作改为顺序写，再结合Zero-Copy的特性极大地改善了IO性能。 而提起磁盘的文件系统，相信很多对硬盘存储了解的同学都知道：“一块SATA RAID-5阵列磁盘的线性写速度可以达到几百M/s，而随机写的速度只能是100多KB/s，线性写的速度是随机写的上千倍”，由此可以看出对磁盘写消息的速度快慢关键还是取决于我们的使用方法。 鉴于此，Kafka的数据存储设计是建立在对文件进行追加的基础上实现的，因为是顺序追加，通过O(1)的磁盘数据结构即可提供消息的持久化，并且这种结构对于即使是数以TB级别的消息存储也能够保持长时间的稳定性能。在理想情况下，只要磁盘空间足够大就一直可以追加消息。此外，Kafka也能够通过配置让用户自己决定已经落盘的持久化消息保存的时间，提供消息处理更为灵活的方式。 Kafka结构层级概述下面将从Kafka文件存储机制和物理结构角度，分析Kafka是如何实现高效文件存储，及实际应用效果。 Kafka部分名词解释如下： （1）Broker：消息中间件处理节点，一个Kafka节点就是一个broker，一个或者多个Broker可以组成一个Kafka集群； （2）Topic：主题是对一组消息的抽象分类，比如例如page view日志、click日志等都可以以topic的形式进行抽象划分类别。在物理上，不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可使得数据的生产者或消费者不必关心数据存于何处； （3）Partition：每个主题又被分成一个或者若干个分区（Partition）。每个分区在本地磁盘上对应一个文件夹，分区命名规则为主题名称后接“—”连接符，之后再接分区编号，分区编号从0开始至分区总数减-1； （4）LogSegment：每个分区又被划分为多个日志分段（LogSegment）组成，日志段是Kafka日志对象分片的最小单位；LogSegment算是一个逻辑概念，对应一个具体的日志文件（“.log”的数据文件）和一个索引文件（“.index”，表示偏移量索引文件）组成；日志文件是一个包含FileMessageSet的文件集实际的消息。索引文件是一个OffsetIndex，它从逻辑偏移量映射到物理文件位置。每个段都有一个基偏移量baseOffset，基偏移量是这个段中的任何消息的最小偏移量，而且大于上一段中的任何偏移量。 （5）Offset：每个partition中都由一系列有序的、不可变的消息组成，这些消息被顺序地追加到partition中。每个消息都有一个连续的序列号称之为offset—偏移量，用于在partition内唯一标识消息（并不表示消息在磁盘上的物理位置）； （6）Message：消息是Kafka中存储的最小最基本的单位，即为一个commit log，由一个固定长度的消息头和一个可变长度的消息体组成； Kafka文件存储机制分析分析过程分为以下4个步骤： topic中partition存储分布 partiton中文件存储方式 partiton中segment文件存储结构 在partition中如何通过offset查找message 通过上述4过程详细分析，我们就可以清楚认识到kafka文件存储机制的奥秘。 ####topic中partition存储分布在三台虚拟机上搭建完成Kafka的集群后（Kafka Broker节点数量为3个），通过在Kafka Broker节点的/bin下执行以下的命令即可创建主题和指定数量的分区以及副本： 1./kafka-topics --create --zookeeper 127.0.0.1:2181 --replication-factor 3 --partitions 3 --topic CommPair 创建完主题、分区和副本后可以查到出主题的状态（该方式主要列举了主题所有分区对应的副本以及ISR列表信息）： 12345./kafka-topics --describe --zookeeper 127.0.0.1:2181 --topic CommPairTopic:CommPair PartitionCount:3 ReplicationFactor:1 Configs: Topic: CommPair Partition: 0 Leader: 60 Replicas: 60 Isr: 60 Topic: CommPair Partition: 1 Leader: 62 Replicas: 62 Isr: 62 Topic: CommPair Partition: 2 Leader: 60 Replicas: 60 Isr: 60 partiton中文件存储方式通过在Kafka的config/server.properties配置文件中“log.dirs”指定的日志数据存储目录下存在三个分区目录，同时在每个分区目录下存在很多对应的日志数据文件和日志索引文件文件，具体如下： 1、分区目录文件123drwxr-x--- 2 root root 4096 Jul 26 19:35 CommPair-0drwxr-x--- 2 root root 4096 Jul 24 20:15 CommPair-1drwxr-x--- 2 root root 4096 Jul 24 20:15 CommPair-2 由上面可以看出，每个分区在物理上对应一个文件夹，分区的命名规则为主题名后接“—”连接符，之后再接分区编号，分区编号从0开始，编号的最大值为分区总数减1。每个分区又有1至多个副本，分区的副本分布在集群的不同代理上，以提高可用性。从存储的角度上来说，分区的每个副本在逻辑上可以抽象为一个日志（Log）对象，即分区副本与日志对象是相对应的。 每个partion(目录)相当于一个巨型文件被平均分配到多个大小相等segment(段)数据文件中。但每个段segment file消息数量不一定相等，这种特性方便old segment file快速被删除。 每个partiton只需要支持顺序读写就行了，segment文件生命周期由服务端配置参数决定。 这样做的好处就是能快速删除无用文件，有效提高磁盘利用率。 partiton中segment文件存储结构 读者从2.2节了解到Kafka文件系统partition存储方式，本节深入分析partion中segment file组成和物理结构。 segment file组成：由2大部分组成，分别为index file和data file，此2个文件一一对应，成对出现，后缀”.index”和“.log”分别表示为segment索引文件、数据文件.segment文件命名规则：partion全局的第一个segment从0开始，后续每个segment文件名为上一个segment文件最后一条消息的offset值。数值最大为64位long大小，19位数字字符长度，没有数字用0填充。下面文件列表是笔者在Kafka broker上做的一个实验，创建一个topicXXX包含1 partition，设置每个segment大小为500MB,并启动producer向Kafka broker写入大量数据,如下图2所示segment文件列表形象说明了上述2个规则： 下图是在三个Kafka Broker节点所组成的集群中分区的主/备份副本的物理分布情况图： 在partition中如何通过offset查找message为了进一步查看“.index”偏移量索引文件、“.log”日志数据文件，可以执行下面的命令将二进制分段的索引和日志数据文件内容转换为字符型文件： 1、执行下面命令即可将日志数据文件内容dump出来12kafka-run-class kafka.tools.DumpLogSegments --files /data1/kafka/data/CommPair-0/00000000001050905938.index --print-data-log &gt;00000000001050905938_txt.logkafka-run-class kafka.tools.DumpLogSegments --files /data1/kafka/data/CommPair-0/00000000001050905938.log --print-data-log &gt;00000000001050905938_txt.index 2、dump出来的具体日志数据内容12345678910111213tail -20 00000000001050905938_txt.logDumping /data1/kafka/data/CommPair-0/00000000001050905938.logStarting offset: 1050905938offset: 1050905938 position: 0 isvalid: true payloadsize: 127 magic: 1 compresscodec: NoCompressionCodec crc: 2108247934 payload: 00000102-363aa04e-4467-41a9-828d-b131281ca9b6^2019-01-22 14:15:32^2019-01-22 14:21:02^UDP^172.16.140.202^49633^224.0.0.252^5355...offset: 1050906189 position: 37479 isvalid: true payloadsize: 126 magic: 1 compresscodec: NoCompressionCodec crc: 2291970542 payload: 00000102-363aa04e-4467-41a9-828d-b131281ca9b6^2019-01-22 14:20:04^2019-01-22 14:26:03^UDP^172.16.140.41^54911^224.0.0.252^5355tail -20 00000000001050905938_txt.indexDumping /data1/kafka/data/CommPair-0/00000000001050905938.indexoffset: 1050906189 position: 37479offset: 1050906505 position: 86271offset: 1050906759 position: 125398offset: 1050907068 position: 172701 由上面dump出来的偏移量索引文件和日志数据文件的具体内容可以分析出来，偏移量索引文件中存储着大量的索引元数据，日志数据文件中存储着大量消息结构中的各个字段内容和消息体本身的值。索引文件中的元数据postion字段指向对应日志数据文件中message的实际位置（即为物理偏移地址）。 1.日志数据文件 Kafka将生产者发送给它的消息数据内容保存至日志数据文件中，该文件以该段的基准偏移量左补齐0命名，文件后缀为“.log”。分区中的每条message由offset来表示它在这个分区中的偏移量，这个offset并不是该Message在分区中实际存储位置，而是逻辑上的一个值（Kafka中用8字节长度来记录这个偏移量），但它却唯一确定了分区中一条Message的逻辑位置，同一个分区下的消息偏移量按照顺序递增（这个可以类比下数据库的自增主键）。另外，从dump出来的日志数据文件的字符值中可以看到消息体的各个字段的内容值。 2.偏移量索引文件 如果消息的消费者每次fetch都需要从1G大小（默认值）的日志数据文件中来查找对应偏移量的消息，那么效率一定非常低，在定位到分段后还需要顺序比对才能找到。Kafka在设计数据存储时，为了提高查找消息的效率，故而为分段后的每个日志数据文件均使用稀疏索引的方式建立索引，这样子既节省空间又能通过索引快速定位到日志数据文件中的消息内容。偏移量索引文件和数据文件一样也同样也以该段的基准偏移量左补齐0命名，文件后缀为“.index”。 从上面dump出来的偏移量索引内容可以看出，索引条目用于将偏移量映射成为消息在日志数据文件中的实际物理位置，每个索引条目由offset和position组成，每个索引条目可以唯一确定在各个分区数据文件的一条消息。其中，Kafka采用稀疏索引存储的方式，每隔一定的字节数建立了一条索引，可以通过“index.interval.bytes”设置索引的跨度； 有了偏移量索引文件，通过它，Kafka就能够根据指定的偏移量快速定位到消息的实际物理位置。具体的做法是，根据指定的偏移量，使用二分法查询定位出该偏移量对应的消息所在的分段索引文件和日志数据文件。然后通过二分查找法，继续查找出小于等于指定偏移量的最大偏移量，同时也得出了对应的position（实际物理位置），根据该物理位置在分段的日志数据文件中顺序扫描查找偏移量与指定偏移量相等的消息。下面是Kafka中分段的日志数据文件和偏移量索引文件的对应映射关系图（其中也说明了如何按照起始偏移量来定位到日志数据文件中的具体消息）。 segment的索引文件中存储着大量的元数据，数据文件中存储着大量消息，索引文件中的元数据指向对应数据文件中的message的物理偏移地址。以索引文件中的3，497为例，在数据文件中表示第3个message（在全局partition表示第368772个message），以及该消息的物理偏移地址为497。 从上述图3了解到segment data file由许多message组成，下面详细说明message物理结构如下： 参数说明： 关键字 解释说明8 byte offset 在parition(分区)内的每条消息都有一个有序的id号，这个id号被称为偏移(offset),它可以唯一确定每条消息在parition(分区)内的位置。即offset表示partiion的第多少message4 byte message size message大小4 byte CRC32 用crc32校验message1 byte “magic” 表示本次发布Kafka服务程序协议版本号1 byte “attributes” 表示为独立版本、或标识压缩类型、或编码类型。4 byte key length 表示key的长度,当key为-1时，K byte key字段不填K byte key 可选value bytes payload 表示实际消息数据。 例如读取offset=368776的message，需要通过下面2个步骤查找。 第一步查找segment file 上述图2为例，其中00000000000000000000.index表示最开始的文件，起始偏移量(offset)为0.第二个文件00000000000000368769.index的消息量起始偏移量为368770 = 368769 + 1.同样，第三个文件00000000000000737337.index的起始偏移量为737338=737337 + 1，其他后续文件依次类推，以起始偏移量命名并排序这些文件，只要根据offset 二分查找 文件列表，就可以快速定位到具体文件。 当offset=368776时定位到00000000000000368769.index|log 第二步通过segment file查找message 通过第一步定位到segment file，当offset=368776时，依次定位到00000000000000368769.index的元数据物理位置和00000000000000368769.log的物理偏移地址，然后再通过00000000000000368769.log顺序查找直到offset=368776为止。 从上述图3可知这样做的优点，segment index file采取稀疏索引存储方式，它减少索引文件大小，通过mmap可以直接内存操作，稀疏索引为数据文件的每个对应message设置一个元数据指针,它比稠密索引节省了更多的存储空间，但查找起来需要消耗更多的时间。 总结Kafka中读写message有如下特点: 写message 消息从java堆转入page cache(即物理内存)。由异步线程刷盘,消息从page cache刷入磁盘。 读message 消息直接从page cache转入socket发送出去。当从page cache没有找到相应数据时，此时会产生磁盘IO,从磁 盘Load消息到page cache,然后直接从socket发出去 Kafka高效文件存储设计特点 Kafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。 通过索引信息可以快速定位message和确定response的最大大小。 通过index元数据全部映射到memory，可以避免segment file的IO磁盘操作。 通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。 参考 Kafka文件存储机制那些事 Kafka之数据存储 Kafka的Log存储解析 消息中间件—Kafka数据存储（一）]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka-最佳实践之-Producer-性能调优（二）]]></title>
    <url>%2F2018%2F11%2F26%2FKafka-%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%E4%B9%8B-Producer-%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Kafka-最佳实践之-Broker-性能调优（一）]]></title>
    <url>%2F2018%2F11%2F26%2FKafka-%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%E4%B9%8B-Broker-%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[通过扩展节点资源，来达到千万TPS等海量吞吐诉求，实际上，我们大部分业务可能不需要那么大规模，对于集群资源有限，如何最大可能调优kafka集群性能，下面我们从broker、producer、consumer 3个方面性能相关参数详细解析，实测解密集群如何最大性能化 网络模型相关参数关于kafka网络模型的原理请查看上一篇博客：Kafka 源码解析之 Server 1+N+M 网络处理模型（一） 参数描述 参数 默认值 说明 num.network.threads 3 Broker用来处理网络请求的网络线程数目；主要处理网络io，读写缓冲区数据，基本没有io等待，配置线程数量为cpu核数加1. num.io.threads 8 broker用来处理请求的I/O线程的数目；主要进行磁盘io操作，高峰期可能有些io等待，因此配置需要大些。配置线程数量为cpu核数2倍，最大不超过3倍 queued.max.requests 500 在网络线程停止读取新请求之前，可以排队等待I/O线程处理的最大请求个数;超过该值，network thread阻塞 Kafka网络通信层的完整框架图如下图所示： Kafka使用NIO自己实现了网络层的代码， 而不是采用netty, mina等第三方的网络框架。从性能上来讲，这一块的代码不是性能的瓶颈。 它采用IO多路复用和多线程下的Reactor模式,主要实现类包括SocketServer, Acceptor, Processor和RequestChannel。 Kafka的服务器由SocketServer实现,它是一个NIO的服务器，线程模型如下： 1个Acceptor线程负责处理新连接 N个Processor线程， 每个processor都有自己的selector，负责从socket中读取请求和发送response M个Handler线程处理请求，并产生response给processor线程 可以从上面的图形中看到Acceptor, Processor和Handler的功能其中num.network.threads是控制上图中的Processor Thread的个数， num.io.threads是控制API Thread的个数，queued.max_requests是控制Request Channel队列的容量 日志保存策略 参数 默认值 说明 log.retention.hours 168 当kafka broker的被写入海量消息后，会生成很多数据文件，占用大量磁盘空间，kafka默认是保留7天，建议根据磁盘情况配置，避免磁盘撑爆 log.segment.bytes 1GB 段文件配置1GB，有利于快速回收磁盘空间，重启kafka加载也会加快(如果文件过小，则文件数量比较多，kafka启动时是单线程扫描目录(log.dir)下所有数据文件) Replica 副本配置 参数 默认值 推荐值 说明 replica.socket.receive.buffer.bytes 64*1024 256k~2M 备份时向leader发送网络请求时的socket receive buffer replica.fetch.max.bytes 1024*1024 根据业务实际配置 备份时每次fetch的最大值 num.replica.fetchers 1 默认值 从leader备份数据的线程数 replica.socket.receive.buffer.bytes： 同TCP buffer的分析，不少于BDP 窗口大小，时延高的可以设大一点 replica.fetch.max.bytes: 不得少于发往broker消息的最大长度（message.max.bytes），需要根据业务实际消息的大小，然后设大一点即可。 num.replica.fetchers： 复制fetch线程设置大一点可以提升获取性能，同时增加备机的IO并行性，但设置太大也没用反而导致空闲，这个同Consumer的fetch thread一样。 在ACK=1的情况下，Produce/Consumer的性能与复制关系不是很大，除非受到网络的瓶颈 参考 kafka性能调优解密（一）– Broker端 Kafka Broker配置（0.10版） Kafka测试及性能调优详细总结 kafka性能参数和压力测试揭秘 kafka性能调优]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka-源码解析之-Producer NIO网络模型（二）]]></title>
    <url>%2F2018%2F11%2F26%2FKafka-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8B-Producer-NIO%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[在写这篇文章之前，专门看了一下 Java NIO 相关的内容，只有理解了 Java NIO 模型才能更好地理解 NIO 在 Kafka 中是如何应用的以及 Producer 如何利用 Java NIO 构建其网络模型（不了解的，可以先看一下上一篇文章：谈一谈 Java IO 模型），同时，本文也是对 Producer 整个流程的一个总结，主要讲述以下两个问题： Producer 的大概网络模型，与 Java NIO 模型之间关系； Producer 整体流程及其整体流程详解。 Producer 的网络模型KafkaProducer 通过 Sender 进行相应的 IO 操作，而 Sender 又调用 NetworkClient 来进行 IO 操作，NetworkClient 底层是对 Java NIO 进行相应的封装，其网络模型如下图所示（该图参考：Kafka源码深度解析－序列3 －Producer －Java NIO，在其基础上增加一个 KafkaProducer 成员变量的图形）。 从图中可以看出，Sender 为最上层的接口，即调用层，Sender 调用 NetworkClient，NetworkClient 调用 Selector，而 Selector 底层封装了 Java NIO 的相关接口，从右边的图也可以看出它们之间的关系。 Producer 整体流程有了对 Producer 网络模型的大概框架认识之后，下面再深入进去，看一下它们之间的调用关系以及 Producer 是如何调用 Java NIO 的相关接口，Producer 端的整体流程如下图所示。 这里涉及到的主要方法是： KafkaProducer.dosend()； Sender.run()； NetworkClient.poll()（NetworkClient.dosend()）； Selector.poll()； 下面会结合上图，对这几个方法做详细的讲解，本文下面的内容都是结合上图进行讲解。 KafkaProducer.dosend()dosend() 方法是 Producer 的入口，dosend() 主要做了两个事情： waitOnMetadata()：请求更新 tp（topic-partition） meta，中间会调用 sender.wakeup()； accumulator.append()：将 msg 写入到其 tp 对应的 deque 中，如果该 tp 对应的 deque 新建了一个 Batch，最后也会调用 sender.wakeup()。这里主要关注的是 sender.wakeup() 方法，它的作用是将 Sender 线程从阻塞中唤醒。 sender.wakeup() 方法这里来看一下 sender.wakeup() 具体实现：123456789101112131415161718192021222324// org.apache.kafka.clients.producer.internals.Sender/*** Wake up the selector associated with this send thread*/public void wakeup() &#123; this.client.wakeup();&#125;// org.apache.kafka.clients.NetworkClient/*** Interrupt the client if it is blocked waiting on I/O.*/@Overridepublic void wakeup() &#123; this.selector.wakeup();&#125;// org.apache.kafka.common.network.Selector/*** Interrupt the nioSelector if it is blocked waiting to do I/O.*///note: 如果 selector 是阻塞的话,就唤醒@Overridepublic void wakeup() &#123; this.nioSelector.wakeup();&#125; 这个方法很简单，但也很有意思，其调用过程是下面这个样子： Sender -&gt; NetworkClient -&gt; Selector(Kafka 封装的) -&gt; Selector(Java NIO) 跟上面两张图中 KafkaProducer 的总体调用过程大概一致，它的作用就是将 Sender 线程从 select() 方法的阻塞中唤醒，select() 方法的作用是轮询注册在多路复用器上的 Channel，它会一直阻塞在这个方法上，除非满足下面条件中的一个： at least one channel is selected; this selector’s {@link #wakeup wakeup} method is invoked; the current thread is interrupted; the given timeout period expires. 否则 select() 将会一直轮询，阻塞在这个地方，直到条件满足。 分析到这里，KafkaProducer 中 dosend() 方法调用 sender.wakeup() 方法作用就很明显的，作用就是：当有新的 RecordBatch 创建后，旧的 RecordBatch 就可以发送了（或者此时有 Metadata 请求需要发送），如果线程阻塞在 select() 方法中，就将其唤醒，Sender 重新开始运行 run() 方法，在这个方法中，旧的 RecordBatch （或相应的 Metadata 请求）将会被选中，进而可以及时将这些请求发送出去。 Sender.run()每次循环都是从 Sender 的 run() 方法开始，具体代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//note: Sender 线程每次循环具体执行的地方 void run(long now) &#123; Cluster cluster = metadata.fetch(); //note: Step1 获取那些已经可以发送的 RecordBatch 对应的 nodes RecordAccumulator.ReadyCheckResult result = this.accumulator.ready(cluster, now); //note: Step2 如果有 topic-partition 的 leader 是未知的,就强制 metadata 更新 if (!result.unknownLeaderTopics.isEmpty()) &#123; for (String topic : result.unknownLeaderTopics) this.metadata.add(topic); this.metadata.requestUpdate(); &#125; //note: 如果与node 没有连接（如果可以连接,会初始化该连接）,暂时先移除该 node Iterator&lt;Node&gt; iter = result.readyNodes.iterator(); long notReadyTimeout = Long.MAX_VALUE; while (iter.hasNext()) &#123; Node node = iter.next(); if (!this.client.ready(node, now)) &#123;//note: 没有建立连接的 broker,这里会与其建立连接 iter.remove(); notReadyTimeout = Math.min(notReadyTimeout, this.client.connectionDelay(node, now)); &#125; &#125; //note: Step3 返回该 node 对应的所有可以发送的 RecordBatch 组成的 batches（key 是 node.id,这些 batches 将会在一个 request 中发送） Map&lt;Integer, List&lt;RecordBatch&gt;&gt; batches = this.accumulator.drain(cluster, result.readyNodes, this.maxRequestSize, now); //note: 保证一个 tp 只有一个 RecordBatch 在发送,保证有序性 //note: max.in.flight.requests.per.connection 设置为1时会保证 if (guaranteeMessageOrder) &#123; // Mute all the partitions draine for (List&lt;RecordBatch&gt; batchList : batches.values()) &#123; for (RecordBatch batch : batchList) this.accumulator.mutePartition(batch.topicPartition); &#125; &#125; //note: 将由于元数据不可用而导致发送超时的 RecordBatch 移除 List&lt;RecordBatch&gt; expiredBatches = this.accumulator.abortExpiredBatches(this.requestTimeout, now); for (RecordBatch expiredBatch : expiredBatches) this.sensors.recordErrors(expiredBatch.topicPartition.topic(), expiredBatch.recordCount); sensors.updateProduceRequestMetrics(batches); long pollTimeout = Math.min(result.nextReadyCheckDelayMs, notReadyTimeout); if (!result.readyNodes.isEmpty()) &#123; log.trace("Nodes with data ready to send: &#123;&#125;", result.readyNodes); pollTimeout = 0; &#125; //note: Step4 发送 RecordBatch sendProduceRequests(batches, now); //note: 如果有 partition 可以立马发送数据,那么 pollTimeout 为0. //note: Step5 关于 socket 的一些实际的读写操作 this.client.poll(pollTimeout, now); &#125; Sender.run() 的大概流程总共有以下五步： accumulator.ready()：遍历所有的 tp（topic-partition），如果其对应的 RecordBatch 可以发送（大小达到 batch.size 大小或时间达到 linger.ms），就将其对应的 leader 选出来，最后会返回一个可以发送 Produce request 的 Set&lt;Node&gt;（实际返回的是 ReadyCheckResult 实例，不过 Set&lt;Node&gt; 是最主要的成员变量）； 如果发现有 tp 没有 leader，那么这里就调用 requestUpdate() 方法更新 metadata，实际上还是在第一步对 tp 的遍历中，遇到没有 leader 的 tp 就将其加入到一个叫做 unknownLeaderTopics 的 set 中，然后会请求这个 tp 的 meta（meta 的更新策略可以参考之前的一篇博客 Producer Metadata 的更新策略）； accumulator.drain()：遍历每个 leader （第一步中选出）上的所有 tp，如果该 tp 对应的 RecordBatch 不在 backoff 期间（没有重试过，或者重试了但是间隔已经达到了 retryBackoffMs ），并且加上这个 RecordBatch 其大小不超过 maxSize（一个 request 的最大限制，默认为 1MB），那么就把这个 RecordBatch 添加 list 中，最终返回的类型为 Map&lt;Integer, List&lt;RecordBatch&gt;&gt;，key 为 leader.id，value 为要发送的 RecordBatch 的列表； sendProduceRequests()：发送 Produce 请求，从图中，可以看出，这个方法会调用 NetworkClient.send() 来发送 clientRequest； NetworkClient.poll()：关于 socket 的 IO 操作都是在这个方法进行的，它还是调用 Selector 进行的相应操作，而 Selector 底层则是封装的 Java NIO 的相关接口，这个下面会详细讲述。 在第三步中，可以看到，如果要向一个 leader 发送 Produce 请求，那么这 leader 对应 tp，如果其 RecordBatch 没有达到要求（batch.size 或 linger.ms 都没达到）还是可能会发送，这样做的好处是：可以减少 request 的频率，有利于提供发送效率。 NetworkClient.poll()这个方法也是一个非常重要的方法，其作用简单来说有三点： 如果需要更新 Metadata，那么就发送 Metadata 请求； 调用 Selector 进行相应的 IO 操作； 处理 Server 端的 response 及一些其他的操作。 具体代码如下所示：123456789101112131415161718192021222324252627282930313233public List&lt;ClientResponse&gt; poll(long timeout, long now) &#123; //note: Step1 判断是否需要更新 meta,如果需要就更新（请求更新 metadata 的地方） long metadataTimeout = metadataUpdater.maybeUpdate(now); //note: Step2 调用 Selector.poll() 进行 socket 相关的 IO 操作 try &#123; this.selector.poll(Utils.min(timeout, metadataTimeout, requestTimeoutMs)); &#125; catch (IOException e) &#123; log.error("Unexpected error during I/O", e); &#125; //note: Step3 处理完成后的操作 long updatedNow = this.time.milliseconds(); List&lt;ClientResponse&gt; responses = new ArrayList&lt;&gt;(); handleAbortedSends(responses); //note: 处理已经完成的 send（不需要 response 的 request,如 send） handleCompletedSends(responses, updatedNow);//note: 通过 selector 中获取 Server 端的 response //note: 处理从 server 端接收到 Receive（如 Metadata 请求） handleCompletedReceives(responses, updatedNow);//note: 在返回的 handler 中，会处理 metadata 的更新 //note: 处理连接失败那些连接,重新请求 meta handleDisconnections(responses, updatedNow); //note: 处理新建立的那些连接（还不能发送请求,比如:还未认证） handleConnections(); handleInitiateApiVersionRequests(updatedNow); handleTimedOutRequests(responses, updatedNow); // invoke callbacks for (ClientResponse response : responses) &#123; try &#123; response.onComplete(); &#125; catch (Exception e) &#123; log.error("Uncaught error in request completion:", e); &#125; &#125; return responses; &#125; 这个方法大致分为三步，这里详述讲述一下： metadataUpdater.maybeUpdate()：如果 Metadata 需要更新，那么就选择连接数最小的 node，发送 Metadata 请求，详细流程可以参考之前那篇博客Producer 的 Metadata 更新流程； selector.poll()：进行 socket IO 相关的操作，下面会详细讲述； process completed actions：在一个 select() 过程之后的相关处理。 handleAbortedSends(responses)：处理那么在发送过程出现 UnsupportedVersionException 异常的 request； handleCompletedSends(responses, updatedNow)：处理那些已经完成的 request，如果是那些不需要 response 的 request 的话，这里直接调用 request.completed()，标志着这个 request 发送处理完成； handleCompletedReceives(responses, updatedNow)：处理那些从 Server 端接收的 Receive，metadata 更新就是在这里处理的（以及 ApiVersionsResponse）； handleDisconnections(responses, updatedNow)：处理连接失败那些连接,重新请求 metadata； handleConnections()：处理新建立的那些连接（还不能发送请求,比如:还未认证）； handleInitiateApiVersionRequests(updatedNow)：对那些新建立的连接，发送 apiVersionRequest（默认情况：第一次建立连接时，需要向 Broker 发送 ApiVersionRequest 请求）； handleTimedOutRequests(responses, updatedNow)：处理 timeout 的连接，关闭该连接，并刷新 Metadata。 Selector.poll() Selector 类是 Kafka 对 Java NIO 相关接口的封装，socket IO 相关的操作都是这个类中完成的，这里先看一下 poll() 方法，主要的操作都是这个方法中调用的，其代码实现如下： 12345678910111213141516171819202122232425262728public void poll(long timeout) throws IOException &#123; if (timeout &lt; 0) throw new IllegalArgumentException("timeout should be &gt;= 0"); //note: Step1 清除相关记录 clear(); if (hasStagedReceives() || !immediatelyConnectedKeys.isEmpty()) timeout = 0; /* check ready keys */ //note: Step2 获取就绪事件的数 long startSelect = time.nanoseconds(); int readyKeys = select(timeout); long endSelect = time.nanoseconds(); this.sensors.selectTime.record(endSelect - startSelect, time.milliseconds()); //note: Step3 处理 io 操作 if (readyKeys &gt; 0 || !immediatelyConnectedKeys.isEmpty()) &#123; pollSelectionKeys(this.nioSelector.selectedKeys(), false, endSelect); pollSelectionKeys(immediatelyConnectedKeys, true, endSelect); &#125; //note: Step4 将处理得到的 stagedReceives 添加到 completedReceives 中 addToCompletedReceives(); long endIo = time.nanoseconds(); this.sensors.ioTime.record(endIo - endSelect, time.milliseconds()); // we use the time at the end of select to ensure that we don't close any connections that // have just been processed in pollSelectionKeys //note: 每次 poll 之后会调用一次 //TODO: 连接虽然关闭了,但是 Client 端的缓存依然存在 maybeCloseOldestConnection(endSelect); &#125; Selector.poll() 方法会进行四步操作，这里分别来介绍一些。 clear() clear() 方法是在每次 poll() 执行的第一步，它作用的就是清理上一次 poll 过程产生的部分缓存。 12345678910111213141516171819//note: 每次 poll 调用前都会清除以下缓存private void clear() &#123; this.completedSends.clear(); this.completedReceives.clear(); this.connected.clear(); this.disconnected.clear(); // Remove closed channels after all their staged receives have been processed or if a send was requested for (Iterator&lt;Map.Entry&lt;String, KafkaChannel&gt;&gt; it = closingChannels.entrySet().iterator(); it.hasNext(); ) &#123; KafkaChannel channel = it.next().getValue(); Deque&lt;NetworkReceive&gt; deque = this.stagedReceives.get(channel); boolean sendFailed = failedSends.remove(channel.id()); if (deque == null || deque.isEmpty() || sendFailed) &#123; doClose(channel, true); it.remove(); &#125; &#125; this.disconnected.addAll(this.failedSends); this.failedSends.clear();&#125; select() Selector 的 select() 方法在实现上底层还是调用 Java NIO 原生的接口，这里的 nioSelector 其实就是 java.nio.channels.Selector 的实例对象，这个方法最坏情况下，会阻塞 ms 的时间，如果在一次轮询，只要有一个 Channel 的事件就绪，它就会立马返回。 12345678private int select(long ms) throws IOException &#123; if (ms &lt; 0L) throw new IllegalArgumentException("timeout should be &gt;= 0"); if (ms == 0L) return this.nioSelector.selectNow(); else return this.nioSelector.select(ms);&#125; pollSelectionKeys() 这部分是 socket IO 的主要部分，发送 Send 及接收 Receive 都是在这里完成的，在 poll() 方法中，这个方法会调用两次： 第一次调用的目的是：处理已经就绪的事件，进行相应的 IO 操作； 第二次调用的目的是：处理新建立的那些连接，添加缓存及传输层（Kafka 又封装了一次，这里后续文章会讲述）的握手与认证。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960private void pollSelectionKeys(Iterable&lt;SelectionKey&gt; selectionKeys, boolean isImmediatelyConnected, long currentTimeNanos) &#123; Iterator&lt;SelectionKey&gt; iterator = selectionKeys.iterator(); while (iterator.hasNext()) &#123; SelectionKey key = iterator.next(); iterator.remove(); KafkaChannel channel = channel(key); // register all per-connection metrics at once sensors.maybeRegisterConnectionMetrics(channel.id()); if (idleExpiryManager != null) idleExpiryManager.update(channel.id(), currentTimeNanos); try &#123; /* complete any connections that have finished their handshake (either normally or immediately) */ //note: 处理一些刚建立 tcp 连接的 channel if (isImmediatelyConnected || key.isConnectable()) &#123; if (channel.finishConnect()) &#123;//note: 连接已经建立 this.connected.add(channel.id()); this.sensors.connectionCreated.record(); SocketChannel socketChannel = (SocketChannel) key.channel(); log.debug("Created socket with SO_RCVBUF = &#123;&#125;, SO_SNDBUF = &#123;&#125;, SO_TIMEOUT = &#123;&#125; to node &#123;&#125;", socketChannel.socket().getReceiveBufferSize(), socketChannel.socket().getSendBufferSize(), socketChannel.socket().getSoTimeout(), channel.id()); &#125; else continue; &#125; /* if channel is not ready finish prepare */ //note: 处理 tcp 连接还未完成的连接,进行传输层的握手及认证 if (channel.isConnected() &amp;&amp; !channel.ready()) channel.prepare(); /* if channel is ready read from any connections that have readable data */ if (channel.ready() &amp;&amp; key.isReadable() &amp;&amp; !hasStagedReceive(channel)) &#123; NetworkReceive networkReceive; while ((networkReceive = channel.read()) != null)//note: 知道读取一个完整的 Receive,才添加到集合中 addToStagedReceives(channel, networkReceive);//note: 读取数据 &#125; /* if channel is ready write to any sockets that have space in their buffer and for which we have data */ if (channel.ready() &amp;&amp; key.isWritable()) &#123; Send send = channel.write(); if (send != null) &#123; this.completedSends.add(send);//note: 将完成的 send 添加到 list 中 this.sensors.recordBytesSent(channel.id(), send.size()); &#125; &#125; /* cancel any defunct sockets */ //note: 关闭断开的连接 if (!key.isValid()) close(channel, true); &#125; catch (Exception e) &#123; String desc = channel.socketDescription(); if (e instanceof IOException) log.debug("Connection with &#123;&#125; disconnected", desc, e); else log.warn("Unexpected error from &#123;&#125;; closing connection", desc, e); close(channel, true); &#125; &#125; &#125; addToCompletedReceives() 这个方法的目的是处理接收到的 Receive，由于 Selector 这个类在 Client 和 Server 端都会调用，这里分两种情况讲述一下： 应用在 Server 端时，后续文章会详细介绍，这里简单说一下，Server 为了保证消息的时序性，在 Selector 中提供了两个方法：mute(String id) 和 unmute(String id)，对该 KafkaChannel 做标记来保证同时只能处理这个 Channel 的一个 request（可以理解为排它锁）。当 Server 端接收到 request 后，先将其放入 stagedReceives 集合中，此时该 Channel 还未 mute，这个 Receive 会被放入 completedReceives 集合中。Server 在对 completedReceives 集合中的 request 进行处理时，会先对该 Channel mute，处理后的 response 发送完成后再对该 Channel unmute，然后才能处理该 Channel 其他的请求； 应用在 Client 端时，Client 并不会调用 Selector 的 mute() 和 unmute() 方法，client 的时序性而是通过 InFlightRequests 和 RecordAccumulator 的 mutePartition 来保证的（下篇文章会讲述），因此对于 Client 端而言，这里接收到的所有 Receive 都会被放入到 completedReceives 的集合中等待后续处理。 这个方法只有配合 Server 端的调用才能看明白其作用，它统一 Client 和 Server 调用的 api，使得都可以使用 Selector 这个类。 1234567891011121314151617181920212223/** * checks if there are any staged receives and adds to completedReceives */private void addToCompletedReceives() &#123; if (!this.stagedReceives.isEmpty()) &#123;//note: 处理 stagedReceives Iterator&lt;Map.Entry&lt;KafkaChannel, Deque&lt;NetworkReceive&gt;&gt;&gt; iter = this.stagedReceives.entrySet().iterator(); while (iter.hasNext()) &#123; Map.Entry&lt;KafkaChannel, Deque&lt;NetworkReceive&gt;&gt; entry = iter.next(); KafkaChannel channel = entry.getKey(); if (!channel.isMute()) &#123; Deque&lt;NetworkReceive&gt; deque = entry.getValue(); addToCompletedReceives(channel, deque); if (deque.isEmpty()) iter.remove(); &#125; &#125; &#125;&#125;private void addToCompletedReceives(KafkaChannel channel, Deque&lt;NetworkReceive&gt; stagedDeque) &#123; NetworkReceive networkReceive = stagedDeque.poll(); this.completedReceives.add(networkReceive); //note: 添加到 completedReceives 中 this.sensors.recordBytesReceived(channel.id(), networkReceive.payload().limit());&#125; Network.send() 方法至此，文章的主要内容已经讲述得差不多了，第二张图中最上面的那个调用关系已经讲述完，下面讲述一下另外一个小分支，也就是从 Sender.run() 调用 NetworkClient.send() 开始的那部分，其调用过程如下：123456Sender.run()Sender.sendProduceRequests()NetworkClient.send()NetworkClient.dosend()Selector.send()KafkaChannel.setSend() NetworkClient.dosend()Producer 端的请求都是通过 NetworkClient.dosend() 来发送的，其作用就是： 检查版本信息，并根据 apiKey() 构建 Request； 创建 NetworkSend 实例； 调用 Selector.send 发送该 Send。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768//note: 发送请求 private void doSend(ClientRequest clientRequest, boolean isInternalRequest, long now) &#123; String nodeId = clientRequest.destination(); if (!isInternalRequest) &#123; // If this request came from outside the NetworkClient, validate // that we can send data. If the request is internal, we trust // that that internal code has done this validation. Validation // will be slightly different for some internal requests (for // example, ApiVersionsRequests can be sent prior to being in // READY state.) if (!canSendRequest(nodeId)) throw new IllegalStateException("Attempt to send a request to node " + nodeId + " which is not ready."); &#125; AbstractRequest request = null; AbstractRequest.Builder&lt;?&gt; builder = clientRequest.requestBuilder(); //note: 构建 AbstractRequest, 检查其版本信息 try &#123; NodeApiVersions versionInfo = nodeApiVersions.get(nodeId); // Note: if versionInfo is null, we have no server version information. This would be // the case when sending the initial ApiVersionRequest which fetches the version // information itself. It is also the case when discoverBrokerVersions is set to false. if (versionInfo == null) &#123; if (discoverBrokerVersions &amp;&amp; log.isTraceEnabled()) log.trace("No version information found when sending message of type &#123;&#125; to node &#123;&#125;. " + "Assuming version &#123;&#125;.", clientRequest.apiKey(), nodeId, builder.version()); &#125; else &#123; short version = versionInfo.usableVersion(clientRequest.apiKey()); builder.setVersion(version); &#125; // The call to build may also throw UnsupportedVersionException, if there are essential // fields that cannot be represented in the chosen version. request = builder.build();//note: 当为 Produce 请求时,转化为 ProduceRequest,Metadata 请求时,转化为 Metadata 请求 &#125; catch (UnsupportedVersionException e) &#123; // If the version is not supported, skip sending the request over the wire. // Instead, simply add it to the local queue of aborted requests. log.debug("Version mismatch when attempting to send &#123;&#125; to &#123;&#125;", clientRequest.toString(), clientRequest.destination(), e); ClientResponse clientResponse = new ClientResponse(clientRequest.makeHeader(), clientRequest.callback(), clientRequest.destination(), now, now, false, e, null); abortedSends.add(clientResponse); return; &#125; RequestHeader header = clientRequest.makeHeader(); if (log.isDebugEnabled()) &#123; int latestClientVersion = ProtoUtils.latestVersion(clientRequest.apiKey().id); if (header.apiVersion() == latestClientVersion) &#123; log.trace("Sending &#123;&#125; to node &#123;&#125;.", request, nodeId); &#125; else &#123; log.debug("Using older server API v&#123;&#125; to send &#123;&#125; to node &#123;&#125;.", header.apiVersion(), request, nodeId); &#125; &#125; //note: Send是一个接口，这里返回的是 NetworkSend，而 NetworkSend 继承 ByteBufferSend Send send = request.toSend(nodeId, header); InFlightRequest inFlightRequest = new InFlightRequest( header, clientRequest.createdTimeMs(), clientRequest.destination(), clientRequest.callback(), clientRequest.expectResponse(), isInternalRequest, send, now); this.inFlightRequests.add(inFlightRequest); //note: 将 send 和对应 kafkaChannel 绑定起来，并开启该 kafkaChannel 底层 socket 的写事件 selector.send(inFlightRequest.send); &#125; Selector.send()这个方法就比较容易理解了，它的作用就是获取该 Send 对应的 KafkaChannel，调用 setSend() 向 KafkaChannel 注册一个 Write 事件。123456789101112131415//note: 发送请求public void send(Send send) &#123; String connectionId = send.destination(); if (closingChannels.containsKey(connectionId)) this.failedSends.add(connectionId); else &#123; KafkaChannel channel = channelOrFail(connectionId, false); try &#123; channel.setSend(send); &#125; catch (CancelledKeyException e) &#123; this.failedSends.add(connectionId); close(channel, false); &#125; &#125;&#125; KafkaChannel.setSend()setSend() 方法需要配合 write()（该方法是在 Selector.poll() 中调用的） 方法一起来看 setSend()：将当前 KafkaChannel 的 Send 赋值为要发送的 Send，并注册一个 OP_WRITE 事件； write()：发送当前的 Send，发送完后删除注册的 OP_WRITE 事件。 1234567891011121314151617181920212223//note: 每次调用时都会注册一个 OP_WRITE 事件public void setSend(Send send) &#123; if (this.send != null) throw new IllegalStateException("Attempt to begin a send operation with prior send operation still in progress."); this.send = send; this.transportLayer.addInterestOps(SelectionKey.OP_WRITE);&#125;//note: 调用 send() 发送 Sendpublic Send write() throws IOException &#123; Send result = null; if (send != null &amp;&amp; send(send)) &#123; result = send; send = null; &#125; return result;&#125;//note: 发送完成后,就删除这个 WRITE 事件private boolean send(Send send) throws IOException &#123; send.writeTo(transportLayer); if (send.completed()) transportLayer.removeInterestOps(SelectionKey.OP_WRITE); return send.completed();&#125; 最后，简单总结一下，可以回过头再看一下第一张图，对于 KafkaProducer 而言，其直接调用是 Sender，而 Sender 底层调用的是 NetworkClient，NetworkClient 则是通过 Selector 实现，Selector 则是对 Java NIO 原生接口的封装。 参考文献： Kafka源码深度解析－序列3 －Producer －Java NIO Kafka源码深度解析－序列4 －Producer －network层核心原理]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka 源码解析之 Server 1+N+M 网络处理模型（一）]]></title>
    <url>%2F2018%2F11%2F24%2FKafka-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8B-Server-1-N-M-%E7%BD%91%E7%BB%9C%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[开源分布式消息队列—Kafka，具备高吞吐量和高并发的特性，其网络通信层是如何做到消息的高效传输的呢？为了解开自己心中的疑虑，就查阅了Kafka的Network通信模块的源码，乘机会写本篇文章。 本文主要通过对Kafka源码的分析来简述其Reactor的多线程网络通信模型和总体框架结构，同时简要介绍Kafka网络通信层的设计与具体实现。 Kafka网络通信模型概述Kafka的网络通信模型是基于NIO的Reactor多线程模型来设计的。这里先引用Kafka源码中注释的一段话： An NIO socket server. The threading model is 1 Acceptor thread that handles new connections. Acceptor has N Processor threads that each have their own selector and read requests from sockets. M Handler threads that handle requests and produce responses back to the processor threads for writing. 相信大家看了上面的这段引文注释后，大致可以了解到Kafka的网络通信层模型，主要采用了1（1个Acceptor线程）+N（N个Processor线程）+M（M个业务处理线程）。下面的表格简要的列举了下（这里先简单的看下后面还会详细说明）： 线程数 线程名 线程具体说明 1 kafka-socket-acceptor_%x Acceptor线程，负责监听Client端发起的请求 N kafka-network-thread_%d Processor线程，负责对Socket进行读写 M kafka-request-handler-_%d Worker线程，处理具体的业务逻辑并生成Response返回 Kafka网络通信层的完整框架图如下图所示： 刚开始看到上面的这个框架图可能会有一些不太理解，并不要紧，这里可以先对Kafka的网络通信层框架结构有一个大致了解。本文后面会结合Kafka的部分重要源码来详细阐述上面的过程。这里可以简单总结一下其网络通信模型中的几个重要概念： Acceptor：1个接收线程，负责监听 Socket 新的连接请求，注册了 OP_ACCEPT 事件，将新的连接按照 round robin 方式交给对应的 Processor 线程处理； Processor：N个处理器线程，其中每个 Processor 都有自己的 selector，它会向 Acceptor 分配的 SocketChannel 注册相应的 OP_READ 事件，N 的大小由num.networker.threads决定； KafkaRequestHandler：M个请求处理线程，包含在线程池—KafkaRequestHandlerPool内部，从RequestChannel的全局请求队列—requestQueue中获取请求数据并交给KafkaApis处理，M的大小由num.io.threads决定； RequestChannel：其为Kafka服务端的请求通道，该数据结构中包含了一个全局的请求队列 requestQueue和多个与Processor处理器相对应的响应队列responseQueue，提供给Processor与请求处理线程KafkaRequestHandler和KafkaApis交换数据的地方。 NetworkClient：其底层是对 Java NIO 进行相应的封装，位于Kafka的网络接口层。Kafka消息生产者对象—KafkaProducer的send方法主要调用NetworkClient完成消息发送； SocketServer：其是一个NIO的服务，它同时启动一个Acceptor接收线程和多个Processor处理器线程。提供了一种典型的Reactor多线程模式，将接收客户端请求和处理请求相分离； KafkaServer：代表了一个Kafka Broker的实例；其startup方法为实例启动的入口； KafkaApis：Kafka的业务逻辑处理Api，负责处理不同类型的请求；比如“发送消息”、“获取消息偏移量—offset”和“处理心跳请求”等； 上图展示的整体的处理流程如下所示： Acceptor 监听到来自请求者（请求者可以是来自 client，也可以来自 server）的新的连接，Acceptor 将这个请求者按照 round robin 的方式交给对对应的 Processor 进行处理； Processor 注册这个 SocketChannel 的 OP_READ 的事件，如果有请求发送过来就可以被 Processor 的 Selector 选中； Processor 将请求者发送的请求放入到一个 Request Queue 中，这是所有 Processor 共有的一个队列； KafkaRequestHandler 从 Request Queue 中取出请求； 调用 KafkaApis 进行相应的处理； 处理的结果放入到该 Processor 对应的 Response Queue 中（每个 request 都标识它们来自哪个 Processor），Request Queue 的数量与 Processor 的数量保持一致； Processor 从对应的 Response Queue 中取出 response； Processor 将处理的结果返回给对应的请求者。 Kafka网络通信层的设计与具体实现 这一节将结合Kafka网络通信层的源码来分析其设计与实现，这里主要详细介绍网络通信层的几个重要元素—SocketServer、Acceptor、Processor、RequestChannel和KafkaRequestHandler。本文分析的源码部分均基于Kafka的0.10.0.1版本。 Server 网络模型整体流程Kafka Server 启动后，会通过 KafkaServer 的 startup() 方法初始化涉及到网络模型的相关对象，如下所示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101 def startup() &#123; try &#123; info("starting") if(isShuttingDown.get) throw new IllegalStateException("Kafka server is still shutting down, cannot re-start!") if(startupComplete.get) return val canStartup = isStartingUp.compareAndSet(false, true) if (canStartup) &#123; metrics = new Metrics(metricConfig, reporters, kafkaMetricsTime, true) brokerState.newState(Starting) /* start scheduler */ kafkaScheduler.startup() /* setup zookeeper */ zkUtils = initZk() /* start log manager */ logManager = createLogManager(zkUtils.zkClient, brokerState) logManager.startup() /* generate brokerId */ config.brokerId = getBrokerId this.logIdent = "[Kafka Server " + config.brokerId + "], " //note: socketServer socketServer = new SocketServer(config, metrics, kafkaMetricsTime) socketServer.startup() /* start replica manager */ replicaManager = new ReplicaManager(config, metrics, time, kafkaMetricsTime, zkUtils, kafkaScheduler, logManager, isShuttingDown) replicaManager.startup() /* start kafka controller */ kafkaController = new KafkaController(config, zkUtils, brokerState, kafkaMetricsTime, metrics, threadNamePrefix) kafkaController.startup() /* start group coordinator */ groupCoordinator = GroupCoordinator(config, zkUtils, replicaManager, kafkaMetricsTime) groupCoordinator.startup() /* Get the authorizer and initialize it if one is specified.*/ authorizer = Option(config.authorizerClassName).filter(_.nonEmpty).map &#123; authorizerClassName =&gt; val authZ = CoreUtils.createObject[Authorizer](authorizerClassName) authZ.configure(config.originals()) authZ &#125; /* start processing requests //NOTE: 初始化 KafkaApis 实例,每个 Server 只会启动一个线程*/ apis = new KafkaApis(socketServer.requestChannel, replicaManager, groupCoordinator, kafkaController, zkUtils, config.brokerId, config, metadataCache, metrics, authorizer) requestHandlerPool = new KafkaRequestHandlerPool(config.brokerId, socketServer.requestChannel, apis, config.numIoThreads) brokerState.newState(RunningAsBroker) Mx4jLoader.maybeLoad() /* start dynamic config manager */ dynamicConfigHandlers = Map[String, ConfigHandler](ConfigType.Topic -&gt; new TopicConfigHandler(logManager, config), ConfigType.Client -&gt; new ClientIdConfigHandler(apis.quotaManagers)) // Apply all existing client configs to the ClientIdConfigHandler to bootstrap the overrides // TODO: Move this logic to DynamicConfigManager AdminUtils.fetchAllEntityConfigs(zkUtils, ConfigType.Client).foreach &#123; case (clientId, properties) =&gt; dynamicConfigHandlers(ConfigType.Client).processConfigChanges(clientId, properties) &#125; // Create the config manager. start listening to notifications dynamicConfigManager = new DynamicConfigManager(zkUtils, dynamicConfigHandlers) dynamicConfigManager.startup() /* tell everyone we are alive */ val listeners = config.advertisedListeners.map &#123;case(protocol, endpoint) =&gt; if (endpoint.port == 0) (protocol, EndPoint(endpoint.host, socketServer.boundPort(protocol), endpoint.protocolType)) else (protocol, endpoint) &#125; kafkaHealthcheck = new KafkaHealthcheck(config.brokerId, listeners, zkUtils, config.rack, config.interBrokerProtocolVersion) kafkaHealthcheck.startup() // Now that the broker id is successfully registered via KafkaHealthcheck, checkpoint it checkpointBrokerId(config.brokerId) /* register broker metrics */ registerStats() shutdownLatch = new CountDownLatch(1) startupComplete.set(true) isStartingUp.set(false) AppInfoParser.registerAppInfo(jmxPrefix, config.brokerId.toString) info("started") &#125; &#125;... &#125; Kafka Server 在启动时会初始化 SocketServer、KafkaApis 和 KafkaRequestHandlerPool 对象，这也是 Server 网络处理模型的主要组成部分。Kafka Server 的网络处理模型也是基于 Java NIO 机制实现的，实现模式与 Reactor 模式类似 上面是 Server 端网络处理的整体流程，下面我们开始详细讲述上面内容在 Kafka 中实现。 SocketServerSocketServer 是接收 Socket 连接、处理请求并返回处理结果的地方，Acceptor 及 Processor 的初始化、处理逻辑都是在这里实现的。在SocketServer 内有几个比较重要的变量，这里先来看下：12345678910111213141516171819class SocketServer(val config: KafkaConfig, val metrics: Metrics, val time: Time, val credentialProvider: CredentialProvider) extends Logging with KafkaMetricsGroup &#123; private val endpoints = config.listeners.map(l =&gt; l.listenerName -&gt; l).toMap //note: broker 开放的端口数 private val numProcessorThreads = config.numNetworkThreads //note: num.network.threads 默认为 3个，即 processor private val maxQueuedRequests = config.queuedMaxRequests //note: queued.max.requests，request 队列中允许的最多请求数，默认是500 private val totalProcessorThreads = numProcessorThreads * endpoints.size //note: 每个端口会对应 N 个 processor private val maxConnectionsPerIp = config.maxConnectionsPerIp //note: 默认 2147483647 private val maxConnectionsPerIpOverrides = config.maxConnectionsPerIpOverrides this.logIdent = "[Socket Server on Broker " + config.brokerId + "], " //note: 请求队列 val requestChannel = new RequestChannel(totalProcessorThreads, maxQueuedRequests) private val processors = new Array[Processor](totalProcessorThreads) private[network] val acceptors = mutable.Map[EndPoint, Acceptor]()&#125;class RequestChannel(val numProcessors: Int, val queueSize: Int) extends KafkaMetricsGroup &#123; private var responseListeners: List[(Int) =&gt; Unit] = Nil //note: 一个 requestQueue 队列,N 个 responseQueues 队列 private val requestQueue = new ArrayBlockingQueue[RequestChannel.Request](queueSize) private val responseQueues = new Array[BlockingQueue[RequestChannel.Response]](numProcessors)&#125; 其中 numProcessorThreads：决定了 Processor 的个数，默认是3个，也就是 1+N+M 的 N 的数值； maxQueuedRequests：决定了 request queue 中最多允许放入多少个请求（等待处理的请求），默认是 500； 在 RequestChannel 中初始化了一个 requestQueue 和 N 个 responseQueue。SocketServer 初始化Boker在启动的时候会调用SocketServer的startup方法，会初始化 1 个 Acceptor 和 N 个 Processor 线程，并启动，其实现如下：1234567891011121314151617181920def startup() &#123; this.synchronized &#123; //note: 一台 broker 一般只设置一个端口，当然这里也可以设置两个 config.listeners.foreach &#123; endpoint =&gt; val listenerName = endpoint.listenerName val securityProtocol = endpoint.securityProtocol val processorEndIndex = processorBeginIndex + numProcessorThreads //note: N 个 processor for (i &lt;- processorBeginIndex until processorEndIndex) processors(i) = newProcessor(i, connectionQuotas, listenerName, securityProtocol) //note: 1个 Acceptor val acceptor = new Acceptor(endpoint, sendBufferSize, recvBufferSize, brokerId, processors.slice(processorBeginIndex, processorEndIndex), connectionQuotas) acceptors.put(endpoint, acceptor) Utils.newThread(s"kafka-socket-acceptor-$listenerName-$securityProtocol-$&#123;endpoint.port&#125;", acceptor, false).start() acceptor.awaitStartup() processorBeginIndex = processorEndIndex &#125; &#125;&#125; AcceptorAcceptor是一个继承自抽象类AbstractServerThread的线程类。Acceptor的主要任务是监听并且接收客户端的请求，同时建立数据传输通道—SocketChannel，然后以轮询的方式交给一个后端的Processor线程处理（具体的方式是添加socketChannel至并发队列并唤醒Processor线程处理）。 在该线程类中主要可以关注以下两个重要的变量： nioSelector：通过NSelector.open()方法创建的变量，封装了JAVA NIO Selector的相关操作； serverChannel：用于监听端口的服务端Socket套接字对象； 实现如下：12345678910111213141516171819202122232425262728293031323334353637383940414243def run() &#123; serverChannel.register(nioSelector, SelectionKey.OP_ACCEPT)//note: 注册 accept 事件 startupComplete() try &#123; var currentProcessor = 0 while (isRunning) &#123; try &#123; val ready = nioSelector.select(500) if (ready &gt; 0) &#123; val keys = nioSelector.selectedKeys() val iter = keys.iterator() while (iter.hasNext &amp;&amp; isRunning) &#123; try &#123; val key = iter.next iter.remove() if (key.isAcceptable) accept(key, processors(currentProcessor))//note: 拿到一个socket 连接，轮询选择一个processor进行处理 else throw new IllegalStateException("Unrecognized key state for acceptor thread.") //note: 轮询算法,使用 round robin // round robin to the next processor thread currentProcessor = (currentProcessor + 1) % processors.length &#125; catch &#123; case e: Throwable =&gt; error("Error while accepting connection", e) &#125; &#125; &#125; &#125; catch &#123; // We catch all the throwables to prevent the acceptor thread from exiting on exceptions due // to a select operation on a specific channel or a bad request. We don't want // the broker to stop responding to requests from other clients in these scenarios. case e: ControlThrowable =&gt; throw e case e: Throwable =&gt; error("Error occurred", e) &#125; &#125; &#125; finally &#123; debug("Closing server socket and selector.") swallowError(serverChannel.close()) swallowError(nioSelector.close()) shutdownComplete() &#125;&#125; Acceptor 通过 accept() 将该新连接交给对应的 Processor，其实现如下：123456789101112131415161718192021222324//note: 处理一个新的连接def accept(key: SelectionKey, processor: Processor) &#123; //note: accept 事件发生时，获取注册到 selector 上的 ServerSocketChannel val serverSocketChannel = key.channel().asInstanceOf[ServerSocketChannel] val socketChannel = serverSocketChannel.accept() try &#123; connectionQuotas.inc(socketChannel.socket().getInetAddress) socketChannel.configureBlocking(false) socketChannel.socket().setTcpNoDelay(true) socketChannel.socket().setKeepAlive(true) if (sendBufferSize != Selectable.USE_DEFAULT_BUFFER_SIZE) socketChannel.socket().setSendBufferSize(sendBufferSize) debug("Accepted connection from %s on %s and assigned it to processor %d, sendBufferSize [actual|requested]: [%d|%d] recvBufferSize [actual|requested]: [%d|%d]" .format(socketChannel.socket.getRemoteSocketAddress, socketChannel.socket.getLocalSocketAddress, processor.id, socketChannel.socket.getSendBufferSize, sendBufferSize, socketChannel.socket.getReceiveBufferSize, recvBufferSize)) //note: 轮询选择不同的 processor 进行处理 processor.accept(socketChannel) &#125; catch &#123; case e: TooManyConnectionsException =&gt; info("Rejected connection from %s, address already has the configured maximum of %d connections.".format(e.ip, e.count)) close(socketChannel) &#125;&#125; 在上面源码中可以看到，Acceptor线程启动后，首先会向用于监听端口的服务端套接字对象—ServerSocketChannel上注册OP_ACCEPT 事件。然后以轮询的方式等待所关注的事件发生。如果该事件发生，则调用accept()方法对OP_ACCEPT事件进行处理。这里，Processor是通过round robin方法选择的，这样可以保证后面多个Processor线程的负载基本均匀。 Acceptor的accept()方法的作用主要如下： 通过SelectionKey取得与之对应的serverSocketChannel实例，并调用它的accept()方法与客户端建立连接； 调用connectionQuotas.inc()方法增加连接统计计数；并同时设置第（1）步中创建返回的socketChannel属性（如sendBufferSize、KeepAlive、TcpNoDelay、configureBlocking等） 将socketChannel交给processor.accept()方法进行处理。这里主要是将socketChannel加入Processor处理器的并发队列newConnections队列中，然后唤醒Processor线程从队列中获取socketChannel并处理。其中，newConnections会被Acceptor线程和Processor线程并发访问操作，所以newConnections是ConcurrentLinkedQueue队列（一个基于链接节点的无界线程安全队列） ProcessorProcessor同Acceptor一样，也是一个线程类，继承了抽象类AbstractServerThread。其主要是从客户端的请求中读取数据和将KafkaRequestHandler处理完响应结果返回给客户端。在该线程类中主要关注以下几个重要的变量： newConnections：在上面的Acceptor一节中已经提到过，它是一种ConcurrentLinkedQueue[SocketChannel]类型的队列，用于保存新连接交由Processor处理的socketChannel； inflightResponses：是一个Map[String, RequestChannel.Response]类型的集合，用于记录尚未发送的响应； selector：是一个类型为KSelector变量，用于管理网络连接；下面先给出Processor处理器线程run方法执行的流程图： 在前面，Acceptor 通过 accept() 将新的连接交给 Processor，Processor 实际上是将该 SocketChannel 添加到该 Processor 的 newConnections 队列中，实现如下：1234def accept(socketChannel: SocketChannel) &#123; newConnections.add(socketChannel)//note: 添加到队列中 wakeup()//note: 唤醒 Processor 的 selector（如果此时在阻塞的话）&#125; 这里详细看下 Processor 线程做了什么事情，其 run() 方法的实现如下：1234567891011121314151617181920212223242526override def run() &#123; startupComplete() while (isRunning) &#123; try &#123; // setup any new connections that have been queued up configureNewConnections()//note: 对新的 socket 连接,并注册 READ 事件 // register any new responses for writing processNewResponses()//note: 处理 response 队列中 response poll() //note: 监听所有的 socket channel，是否有新的请求发送过来 processCompletedReceives() //note: 处理接收到请求，将其放入到 request queue 中 processCompletedSends() //note: 处理已经完成的发送 processDisconnected() //note: 处理断开的连接 &#125; catch &#123; // We catch all the throwables here to prevent the processor thread from exiting. We do this because // letting a processor exit might cause a bigger impact on the broker. Usually the exceptions thrown would // be either associated with a specific socket channel or a bad request. We just ignore the bad socket channel // or request. This behavior might need to be reviewed if we see an exception that need the entire broker to stop. case e: ControlThrowable =&gt; throw e case e: Throwable =&gt; error("Processor got uncaught exception.", e) &#125; &#125; debug("Closing selector - processor " + id) swallowError(closeAll()) shutdownComplete()&#125; Processor 在一次循环中，主要做的事情如下： configureNewConnections()：对新添加到 newConnections 队列中的 SocketChannel 进行处理，这里主要是遍历取出队列中的每个socketChannel并将其在selector上注册OP_READ事件； processNewResponses()：从该 Processor 对应的 response queue 中取出一个 response，进行发送,在这一步中会根据responseAction的类型（NoOpAction/SendAction/CloseConnectionAction）进行判断，若为“NoOpAction”，表示该连接对应的请求无需响应；若为“SendAction”，表示该Response需要发送给客户端，则会通过“selector.send”注册OP_WRITE事件，并且将该Response从responseQueue响应队列中移至inflightResponses集合中；“CloseConnectionAction”，表示该连接是要关闭的；； poll()：调用 selector 的 poll() 方法，遍历注册的 SocketChannel，查看是否有事件准备就绪； processCompletedReceives()：将接收到请求添加到的 request queue 中,在processCompletedReceives方法中调用“requestChannel.sendRequest”方法将请求Request添加至requestChannel的全局请求队列—requestQueue中，等待KafkaRequestHandler来处理。同时，调用“selector.mute”方法取消与该请求对应的连接通道上的OP_READ事件； processCompletedSends()：处理已经完成的响应发送,当已经完成将response发送给客户端，则将其从inflightResponses移除，同时通过调用“selector.unmute”方法为对应的连接通道重新注册OP_READ事件； processDisconnected()：处理断开的 SocketChannel, 将该response从inflightResponses集合中移除，同时将connectionQuotas统计计数减1。 上面就是 Processor 线程处理的主要逻辑，先是向新的 SocketChannel 注册相应的事件，监控是否有请求发送过来，接着从 response queue 中取出处理完成的请求发送给对应的请求者，然后调用一下 selector 的 poll()，遍历一下注册的所有 SocketChannel，判断是否有事件就绪，然后做相应的处理。这里需要注意的是，request queue 是所有 Processor 公用的一个队列，而 response queue 则是与 Processor 一一对应的，因为每个 Processor 监听的 SocketChannel 并不是同一批的，如果公有一个 response queue，那么这个 N 个 Processor 的 selector 要去监听所有的 SocketChannel，而不是现在这种，只需要去关注分配给自己的 SocketChannel。 下面分别看下上面的这些方法的具体实现。 configureNewConnectionsconfigureNewConnections() 对新添加到 newConnections 队列中的 SocketChannel 进行处理，主要是 selector 注册相应的 OP_READ 事件，其实现如下：1234567891011121314151617181920212223//note: 如果有新的连接过来，将该 Channel 的 OP_READ 事件注册到 selector 上private def configureNewConnections() &#123; while (!newConnections.isEmpty) &#123; val channel = newConnections.poll() try &#123; debug(s"Processor $id listening to new connection from $&#123;channel.socket.getRemoteSocketAddress&#125;") val localHost = channel.socket().getLocalAddress.getHostAddress val localPort = channel.socket().getLocalPort val remoteHost = channel.socket().getInetAddress.getHostAddress val remotePort = channel.socket().getPort val connectionId = ConnectionId(localHost, localPort, remoteHost, remotePort).toString selector.register(connectionId, channel) &#125; catch &#123; // We explicitly catch all non fatal exceptions and close the socket to avoid a socket leak. The other // throwables will be caught in processor and logged as uncaught exceptions. case NonFatal(e) =&gt; val remoteAddress = channel.getRemoteAddress // need to close the channel here to avoid a socket leak. close(channel) error(s"Processor $id closed connection from $remoteAddress", e) &#125; &#125;&#125; processNewResponsesprocessNewResponses() 方法是从该 Processor 对应的 response queue 中取出一个 response，Processor 是通过 RequestChannel 的 receiveResponse() 从该 Processor 对应的 response queue 中取出 response，如下所示：1234567//note: 获取 responsedef receiveResponse(processor: Int): RequestChannel.Response = &#123; val response = responseQueues(processor).poll() if (response != null) response.request.responseDequeueTimeMs = Time.SYSTEM.milliseconds response&#125; 取到相应的 response 之后，会判断该 response 的类型，进行相应的操作，如果需要返回，那么会调用 sendResponse() 发送该 response，如下所示：1234567891011121314151617181920212223242526272829303132333435363738394041//note: 处理一个新的 response 响应private def processNewResponses() &#123; var curr = requestChannel.receiveResponse(id) while (curr != null) &#123; try &#123; curr.responseAction match &#123; case RequestChannel.NoOpAction =&gt; //note: 如果这个请求不需要返回 response，再次注册该监听事件 // There is no response to send to the client, we need to read more pipelined requests // that are sitting in the server's socket buffer curr.request.updateRequestMetrics trace("Socket server received empty response to send, registering for read: " + curr) val channelId = curr.request.connectionId if (selector.channel(channelId) != null || selector.closingChannel(channelId) != null) selector.unmute(channelId) case RequestChannel.SendAction =&gt; //note: 需要发送的 response，那么进行发送 sendResponse(curr) case RequestChannel.CloseConnectionAction =&gt; //note: 要关闭的 response curr.request.updateRequestMetrics trace("Closing socket connection actively according to the response code.") close(selector, curr.request.connectionId) &#125; &#125; finally &#123; curr = requestChannel.receiveResponse(id) &#125; &#125;&#125;/* `protected` for test usage *///note: 发送的对应的 responseprotected[network] def sendResponse(response: RequestChannel.Response) &#123; trace(s"Socket server received response to send, registering for write and sending data: $response") val channel = selector.channel(response.responseSend.destination) // `channel` can be null if the selector closed the connection because it was idle for too long if (channel == null) &#123; warn(s"Attempting to send response via channel for which there is no open connection, connection id $id") response.request.updateRequestMetrics() &#125; else &#123; selector.send(response.responseSend) //note: 发送该 response inflightResponses += (response.request.connectionId -&gt; response) //note: 添加到 inflinght 中 &#125;&#125; processCompletedReceivesprocessCompletedReceives()方法的主要作用是处理接收到请求，并将其放入到 request queue 中，其实现如下：1234567891011121314151617181920212223//note: 处理接收到的所有请求private def processCompletedReceives() &#123; selector.completedReceives.asScala.foreach &#123; receive =&gt; try &#123; val openChannel = selector.channel(receive.source) val session = &#123; // Only methods that are safe to call on a disconnected channel should be invoked on 'channel'. val channel = if (openChannel != null) openChannel else selector.closingChannel(receive.source) RequestChannel.Session(new KafkaPrincipal(KafkaPrincipal.USER_TYPE, channel.principal.getName), channel.socketAddress) &#125; val req = RequestChannel.Request(processor = id, connectionId = receive.source, session = session, buffer = receive.payload, startTimeMs = time.milliseconds, listenerName = listenerName, securityProtocol = securityProtocol) requestChannel.sendRequest(req) //note: 添加到请求队列，如果队列满了，将会阻塞 selector.mute(receive.source) //note: 移除该连接的 OP_READ 监听 &#125; catch &#123; case e @ (_: InvalidRequestException | _: SchemaException) =&gt; // note that even though we got an exception, we can assume that receive.source is valid. Issues with constructing a valid receive object were handled earlier error(s"Closing socket for $&#123;receive.source&#125; because of error", e) close(selector, receive.source) &#125; &#125;&#125; processCompletedSendsprocessCompletedSends() 方法是处理已经完成的发送，其实现如下：12345678910private def processCompletedSends() &#123; selector.completedSends.asScala.foreach &#123; send =&gt; //note: response 发送完成，从正在发送的集合中移除 val resp = inflightResponses.remove(send.destination).getOrElse &#123; throw new IllegalStateException(s"Send for $&#123;send.destination&#125; completed, but not in `inflightResponses`") &#125; resp.request.updateRequestMetrics() selector.unmute(send.destination) //note: 完成这个请求之后再次监听 OP_READ 事件 &#125;&#125; KafkaRequestHandlerPool上面主要是讲述 SocketServer 中 Acceptor 与 Processor 的处理内容，也就是 1+N+M 模型中 1+N 部分，下面开始讲述 M 部分，也就是 KafkaRequestHandler 的内容，其初始化实现如下：1234567891011121314151617181920212223242526class KafkaRequestHandlerPool(val brokerId: Int, val requestChannel: RequestChannel, val apis: KafkaApis, time: Time, numThreads: Int) extends Logging with KafkaMetricsGroup &#123; /* a meter to track the average free capacity of the request handlers */ private val aggregateIdleMeter = newMeter("RequestHandlerAvgIdlePercent", "percent", TimeUnit.NANOSECONDS) this.logIdent = "[Kafka Request Handler on Broker " + brokerId + "], " val threads = new Array[Thread](numThreads) val runnables = new Array[KafkaRequestHandler](numThreads) //note: 建立 M 个（numThreads）KafkaRequestHandler for(i &lt;- 0 until numThreads) &#123; //note: requestChannel 是 Processor 存放 request 请求的地方,也是 Handler 处理完请求存放 response 的地方 runnables(i) = new KafkaRequestHandler(i, brokerId, aggregateIdleMeter, numThreads, requestChannel, apis, time) threads(i) = Utils.daemonThread("kafka-request-handler-" + i, runnables(i)) threads(i).start() &#125; def shutdown() &#123; info("shutting down") for(handler &lt;- runnables) handler.shutdown for(thread &lt;- threads) thread.join info("shut down completely") &#125;&#125; 如上面实现所示： KafkaRequestHandlerPool 会初始化 M 个 KafkaRequestHandler 线程，并启动该线程； 在初始化 KafkaRequestHandler 时，传入一个 requestChannel 变量，这个是 Processor 存放 request 的地方，KafkaRequestHandler 在处理请求时，会从这个 queue 中取出相应的 request。KafkaRequestHandlerKafkaRequestHandler 线程的处理实现如下：123456789101112131415161718192021222324252627def run() &#123; while(true) &#123; try &#123; var req : RequestChannel.Request = null while (req == null) &#123; // We use a single meter for aggregate idle percentage for the thread pool. // Since meter is calculated as total_recorded_value / time_window and // time_window is independent of the number of threads, each recorded idle // time should be discounted by # threads. val startSelectTime = time.nanoseconds req = requestChannel.receiveRequest(300) //note: 从 request queue 中拿去 request val idleTime = time.nanoseconds - startSelectTime aggregateIdleMeter.mark(idleTime / totalHandlerThreads) &#125; if(req eq RequestChannel.AllDone) &#123; debug("Kafka request handler %d on broker %d received shut down command".format( id, brokerId)) return &#125; req.requestDequeueTimeMs = time.milliseconds trace("Kafka request handler %d on broker %d handling request %s".format(id, brokerId, req)) apis.handle(req) //note: 处理请求,并将处理的结果通过 sendResponse 放入 response queue 中 &#125; catch &#123; case e: Throwable =&gt; error("Exception when handling request", e) &#125; &#125;&#125; 上述方法的实现逻辑： KafkaRequestHandler不断的从requestChannel队列里面取出request交给apis处理； KafkaApis 处理这个 request，并通过 requestChannel.sendResponse() 将处理的结果放入 requestChannel 的 response queue 中，如下所示：123456//note: 将 response 添加到对应的队列中def sendResponse(response: RequestChannel.Response) &#123; responseQueues(response.processor).put(response) for(onResponse &lt;- responseListeners) onResponse(response.processor) //note: 调用对应 processor 的 wakeup 方法&#125; KafkaRequestHandler也是一种线程类，在KafkaServer实例启动时候会实例化一个线程池—KafkaRequestHandlerPool对象（包含了若干个KafkaRequestHandler线程），这些线程以守护线程的方式在后台运行。在KafkaRequestHandler的run方法中会循环地从RequestChannel中阻塞式读取request，读取后再交由KafkaApis来具体处理。 KafkaApisKafkaApis是用于处理对通信网络传输过来的业务消息请求的中心转发组件。该组件反映出Kafka Broker Server可以提供哪些服务。apis根据不同的请求类型调用不同的方法进行处理， 代码如下：12345678910111213141516171819202122232425262728293031/** * Top-level method that handles all requests and multiplexes to the right api */ def handle(request: RequestChannel.Request) &#123; try &#123; ApiKeys.forId(request.requestId) match &#123; case ApiKeys.PRODUCE =&gt; handleProducerRequest(request) case ApiKeys.FETCH =&gt; handleFetchRequest(request) case ApiKeys.LIST_OFFSETS =&gt; handleOffsetRequest(request) case ApiKeys.METADATA =&gt; handleTopicMetadataRequest(request) case ApiKeys.LEADER_AND_ISR =&gt; handleLeaderAndIsrRequest(request) case ApiKeys.STOP_REPLICA =&gt; handleStopReplicaRequest(request) case ApiKeys.UPDATE_METADATA_KEY =&gt; handleUpdateMetadataRequest(request) case ApiKeys.CONTROLLED_SHUTDOWN_KEY =&gt; handleControlledShutdownRequest(request) case ApiKeys.OFFSET_COMMIT =&gt; handleOffsetCommitRequest(request) case ApiKeys.OFFSET_FETCH =&gt; handleOffsetFetchRequest(request) case ApiKeys.GROUP_COORDINATOR =&gt; handleGroupCoordinatorRequest(request) case ApiKeys.JOIN_GROUP =&gt; handleJoinGroupRequest(request) case ApiKeys.HEARTBEAT =&gt; handleHeartbeatRequest(request) case ApiKeys.LEAVE_GROUP =&gt; handleLeaveGroupRequest(request) case ApiKeys.SYNC_GROUP =&gt; handleSyncGroupRequest(request) case ApiKeys.DESCRIBE_GROUPS =&gt; handleDescribeGroupRequest(request) case ApiKeys.LIST_GROUPS =&gt; handleListGroupsRequest(request) case ApiKeys.SASL_HANDSHAKE =&gt; handleSaslHandshakeRequest(request) case ApiKeys.API_VERSIONS =&gt; handleApiVersionsRequest(request) case requestId =&gt; throw new KafkaException("Unknown api code " + requestId) &#125; &#125;catch &#123; &#125; &#125; finally request.apiLocalCompleteTimeMs = SystemTime.milliseconds 显然，此处处理的速度影响Kafka整体的消息处理的速度。这里我们分析一个处理方法handleProducerRequest。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/** * Handle a produce request */ def handleProducerRequest(request: RequestChannel.Request) &#123; val produceRequest = request.body.asInstanceOf[ProduceRequest] // the callback for sending a produce response def sendResponseCallback(responseStatus: Map[TopicPartition, PartitionResponse]) &#123; var errorInResponse = false mergedResponseStatus.foreach &#123; case (topicPartition, status) =&gt; if (status.errorCode != Errors.NONE.code) &#123; errorInResponse = true &#125; &#125; def produceResponseCallback(delayTimeMs: Int) &#123; if (produceRequest.acks == 0) &#123; // no operation needed if producer request.required.acks = 0; however, if there is any error in handling // the request, since no response is expected by the producer, the server will close socket server so that // the producer client will know that some error has happened and will refresh its metadata if (errorInResponse) &#123; val exceptionsSummary = mergedResponseStatus.map &#123; case (topicPartition, status) =&gt; topicPartition -&gt; Errors.forCode(status.errorCode).exceptionName &#125;.mkString(", ") requestChannel.closeConnection(request.processor, request) &#125; else &#123; requestChannel.noOperation(request.processor, request) &#125; &#125; else &#123; val respHeader = new ResponseHeader(request.header.correlationId) val respBody = request.header.apiVersion match &#123; case 0 =&gt; new ProduceResponse(mergedResponseStatus.asJava) case version@(1 | 2) =&gt; new ProduceResponse(mergedResponseStatus.asJava, delayTimeMs, version) // This case shouldn't happen unless a new version of ProducerRequest is added without // updating this part of the code to handle it properly. case version =&gt; throw new IllegalArgumentException(s"Version `$version` of ProduceRequest is not handled. Code must be updated.") &#125; requestChannel.sendResponse(new RequestChannel.Response(request, new ResponseSend(request.connectionId, respHeader, respBody))) &#125; &#125; // When this callback is triggered, the remote API call has completed request.apiRemoteCompleteTimeMs = SystemTime.milliseconds quotaManagers(ApiKeys.PRODUCE.id).recordAndMaybeThrottle( request.header.clientId, numBytesAppended, produceResponseCallback) &#125; if (authorizedRequestInfo.isEmpty) sendResponseCallback(Map.empty) else &#123; val internalTopicsAllowed = request.header.clientId == AdminUtils.AdminClientId // Convert ByteBuffer to ByteBufferMessageSet val authorizedMessagesPerPartition = authorizedRequestInfo.map &#123; case (topicPartition, buffer) =&gt; (topicPartition, new ByteBufferMessageSet(buffer)) &#125; // call the replica manager to append messages to the replicas replicaManager.appendMessages( produceRequest.timeout.toLong, produceRequest.acks, internalTopicsAllowed, authorizedMessagesPerPartition, sendResponseCallback) // if the request is put into the purgatory, it will have a held reference // and hence cannot be garbage collected; hence we clear its data here in // order to let GC re-claim its memory since it is already appended to log produceRequest.clearPartitionRecords() &#125; &#125; 这里会调用replicaManager.appendMessages处理Kafka message的保存和备份,也就是leader和备份节点上。 Replication Subsystem顺藤摸瓜，我们进入replicaManager.appendMessages的代码。这个方法会将消息放到leader分区上，并复制到备份分区上。在超时或者根据required acks的值及时返回response。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * Append messages to leader replicas of the partition, and wait for them to be replicated to other replicas; * the callback function will be triggered either when timeout or the required acks are satisfied */ def appendMessages(timeout: Long, requiredAcks: Short, internalTopicsAllowed: Boolean, messagesPerPartition: Map[TopicPartition, MessageSet], responseCallback: Map[TopicPartition, PartitionResponse] =&gt; Unit) &#123; if (isValidRequiredAcks(requiredAcks)) &#123; val sTime = SystemTime.milliseconds val localProduceResults = appendToLocalLog(internalTopicsAllowed, messagesPerPartition, requiredAcks) debug("Produce to local log in %d ms".format(SystemTime.milliseconds - sTime)) val produceStatus = localProduceResults.map &#123; case (topicPartition, result) =&gt; topicPartition -&gt; ProducePartitionStatus( result.info.lastOffset + 1, // required offset new PartitionResponse(result.errorCode, result.info.firstOffset, result.info.timestamp)) // response status &#125; if (delayedRequestRequired(requiredAcks, messagesPerPartition, localProduceResults)) &#123; // create delayed produce operation val produceMetadata = ProduceMetadata(requiredAcks, produceStatus) val delayedProduce = new DelayedProduce(timeout, produceMetadata, this, responseCallback) // create a list of (topic, partition) pairs to use as keys for this delayed produce operation val producerRequestKeys = messagesPerPartition.keys.map(new TopicPartitionOperationKey(_)).toSeq // try to complete the request immediately, otherwise put it into the purgatory // this is because while the delayed produce operation is being created, new // requests may arrive and hence make this operation completable. delayedProducePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys) &#125; else &#123; // we can respond immediately val produceResponseStatus = produceStatus.mapValues(status =&gt; status.responseStatus) responseCallback(produceResponseStatus) &#125; &#125; else &#123; // If required.acks is outside accepted range, something is wrong with the client // Just return an error and don't handle the request at all val responseStatus = messagesPerPartition.map &#123; case (topicAndPartition, messageSet) =&gt; (topicAndPartition -&gt; new PartitionResponse(Errors.INVALID_REQUIRED_ACKS.code, LogAppendInfo.UnknownLogAppendInfo.firstOffset, Message.NoTimestamp)) &#125; responseCallback(responseStatus) &#125; &#125; Log SubsystemLogManager负责管理Kafka的Log(Kafka消息)， 包括log/Log文件夹的创建，获取和清理。它也会通过定时器检查内存中的log是否要缓存到磁盘中。重要的类包括LogManager和Log。 OffsetManager负责管理offset，提供offset的读写。 TopicConfigManager它负责动态改变Topic的配置属性。如果某个topic的配置属性改变了，Kafka会在ZooKeeper上创建一个类似/brokers/config_changes/config_change_13321的节点， topicConfigManager会监控这些节点， 获得属性改变的topics并处理,实际上以新的LogConfig替换老的 RequestChannel在Kafka的网络通信层中，RequestChannel为Processor处理器线程与KafkaRequestHandler线程之间的数据交换提供了一个数据缓冲区，是通信过程中Request和Response缓存的地方。因此，其作用就是在通信中起到了一个数据缓冲队列的作用。Processor线程将读取到的请求添加至RequestChannel的全局请求队列—requestQueue中；KafkaRequestHandler线程从请求队列中获取并处理，处理完以后将Response添加至RequestChannel的响应队列—responseQueue中，并通过responseListeners唤醒对应的Processor线程，最后Processor线程从响应队列中取出后发送至客户端。 到这里为止，一个请求从 Processor 接收，到 KafkaRequestHandler 通过 KafkaApis 处理并放回该 Processor 对应的 response queue 这整个过程就完成了（建议阅读本文的时候结合最前面的流程图一起看）。 参考消息中间件—简谈Kafka中的NIO网络通信模型 kafka源码解析之八：Broker分析]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
</search>
